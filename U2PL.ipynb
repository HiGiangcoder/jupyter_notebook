{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "notebook263586676f",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HiGiangcoder/jupyter_notebook/blob/master/U2PL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import copy\n",
        "import logging\n",
        "import os\n",
        "import os.path as osp\n",
        "import pprint\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.distributed as dist\n",
        "import torch.nn.functional as F\n",
        "import yaml\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "from u2pl.dataset.augmentation import generate_unsup_data\n",
        "from u2pl.dataset.builder import get_loader\n",
        "from u2pl.models.model_helper import ModelBuilder\n",
        "from u2pl.utils.dist_helper import setup_distributed\n",
        "from u2pl.utils.loss_helper import (\n",
        "    compute_contra_memobank_loss,\n",
        "    compute_unsupervised_loss,\n",
        "    get_criterion,\n",
        ")\n",
        "from u2pl.utils.lr_helper import get_optimizer, get_scheduler\n",
        "from u2pl.utils.utils import (\n",
        "    AverageMeter,\n",
        "    get_rank,\n",
        "    get_world_size,\n",
        "    init_log,\n",
        "    intersectionAndUnion,\n",
        "    label_onehot,\n",
        "    load_state,\n",
        "    set_random_seed,\n",
        ")\n",
        "\n",
        "parser = argparse.ArgumentParser(description=\"Semi-Supervised Semantic Segmentation\")\n",
        "parser.add_argument(\"--config\", type=str, default=\"config.yaml\")\n",
        "parser.add_argument(\"--local_rank\", type=int, default=0)\n",
        "parser.add_argument(\"--seed\", type=int, default=0)\n",
        "parser.add_argument(\"--port\", default=None, type=int)\n",
        "\n",
        "\n",
        "def main():\n",
        "    global args, cfg, prototype\n",
        "    args = parser.parse_args()  # Phân tích các tham số dòng lệnh được truyền vào chương trình.\n",
        "    seed = args.seed  # Lấy giá trị seed từ tham số dòng lệnh để thiết lập ngẫu nhiên.\n",
        "    cfg = yaml.load(open(args.config, \"r\"), Loader=yaml.Loader)  # Tải cấu hình từ file YAML được chỉ định.\n",
        "\n",
        "    logger = init_log(\"global\", logging.INFO)  # Khởi tạo logger để ghi log với mức độ INFO.\n",
        "    logger.propagate = 0  # Ngăn logger ghi log lên các logger cha.\n",
        "\n",
        "    cfg[\"exp_path\"] = os.path.dirname(args.config)  # Lưu đường dẫn thư mục chứa file cấu hình.\n",
        "    cfg[\"save_path\"] = os.path.join(cfg[\"exp_path\"], cfg[\"saver\"][\"snapshot_dir\"])  # Tạo đường dẫn lưu kết quả.\n",
        "\n",
        "    cudnn.enabled = True  # Bật tăng tốc cuDNN để tăng hiệu năng khi sử dụng GPU.\n",
        "    cudnn.benchmark = True  # Cho phép cuDNN tối ưu hóa hiệu năng dựa trên kích thước batch cố định.\n",
        "\n",
        "    rank, word_size = setup_distributed(port=args.port)  # Thiết lập môi trường phân tán và lấy rank, world size.\n",
        "\n",
        "    if rank == 0:  # Nếu là tiến trình chính (rank 0):\n",
        "        logger.info(\"{}\".format(pprint.pformat(cfg)))  # Ghi log cấu hình đã tải.\n",
        "        current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")  # Lấy thời gian hiện tại để đặt tên thư mục log.\n",
        "        tb_logger = SummaryWriter(\n",
        "            osp.join(cfg[\"exp_path\"], \"log/events_seg/\" + current_time)\n",
        "        )  # Khởi tạo logger TensorBoard để ghi log.\n",
        "    else:\n",
        "        tb_logger = None  # Các tiến trình khác không cần ghi log TensorBoard.\n",
        "\n",
        "    if args.seed is not None:\n",
        "        print(\"set random seed to\", args.seed)  # In ra seed được sử dụng.\n",
        "        set_random_seed(args.seed)  # Thiết lập seed để đảm bảo tính tái lập.\n",
        "\n",
        "    if not osp.exists(cfg[\"saver\"][\"snapshot_dir\"]) and rank == 0:\n",
        "        os.makedirs(cfg[\"saver\"][\"snapshot_dir\"])  # Tạo thư mục lưu checkpoint nếu chưa tồn tại.\n",
        "\n",
        "    # Tạo mô hình chính (student model)\n",
        "    model = ModelBuilder(cfg[\"net\"])  # Xây dựng mô hình dựa trên cấu hình trong file YAML.\n",
        "    modules_back = [model.encoder]  # Encoder của mô hình (tương tự UNet).\n",
        "    if cfg[\"net\"].get(\"aux_loss\", False):  # Nếu sử dụng auxiliary loss:\n",
        "        modules_head = [model.auxor, model.decoder]  # Thêm auxiliary head.\n",
        "    else:\n",
        "        modules_head = [model.decoder]  # Chỉ sử dụng decoder.\n",
        "\n",
        "    if cfg[\"net\"].get(\"sync_bn\", True):  # Nếu sử dụng synchronized batch normalization:\n",
        "        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
        "\n",
        "    model.cuda()  # Đưa mô hình lên GPU.\n",
        "\n",
        "    sup_loss_fn = get_criterion(cfg)  # Hàm tính loss cho dữ liệu có nhãn.\n",
        "\n",
        "    train_loader_sup, train_loader_unsup, val_loader = get_loader(cfg, seed=seed)  # Tải dữ liệu huấn luyện và validation.\n",
        "\n",
        "    # Tạo optimizer và scheduler cho learning rate\n",
        "    cfg_trainer = cfg[\"trainer\"]\n",
        "    cfg_optim = cfg_trainer[\"optimizer\"]\n",
        "    times = 10 if \"pascal\" in cfg[\"dataset\"][\"type\"] else 1  # Tăng learning rate cho các module head nếu dataset là Pascal.\n",
        "\n",
        "    params_list = []\n",
        "    for module in modules_back:  # Thêm các tham số của encoder vào optimizer.\n",
        "        params_list.append(\n",
        "            dict(params=module.parameters(), lr=cfg_optim[\"kwargs\"][\"lr\"])\n",
        "        )\n",
        "    for module in modules_head:  # Thêm các tham số của decoder và auxiliary head vào optimizer.\n",
        "        params_list.append(\n",
        "            dict(params=module.parameters(), lr=cfg_optim[\"kwargs\"][\"lr\"] * times)\n",
        "        )\n",
        "\n",
        "    optimizer = get_optimizer(params_list, cfg_optim)  # Khởi tạo optimizer.\n",
        "\n",
        "    local_rank = int(os.environ[\"LOCAL_RANK\"])  # Lấy rank của tiến trình hiện tại.\n",
        "    model = torch.nn.parallel.DistributedDataParallel(\n",
        "        model,\n",
        "        device_ids=[local_rank],\n",
        "        output_device=local_rank,\n",
        "        find_unused_parameters=False,\n",
        "    )  # Sử dụng DistributedDataParallel để huấn luyện phân tán.\n",
        "\n",
        "    # Tạo mô hình giáo viên (teacher model)\n",
        "    model_teacher = ModelBuilder(cfg[\"net\"])  # Xây dựng mô hình giáo viên.\n",
        "    model_teacher = model_teacher.cuda()  # Đưa mô hình giáo viên lên GPU.\n",
        "    model_teacher = torch.nn.parallel.DistributedDataParallel(\n",
        "        model_teacher,\n",
        "        device_ids=[local_rank],\n",
        "        output_device=local_rank,\n",
        "        find_unused_parameters=False,\n",
        "    )\n",
        "\n",
        "    for p in model_teacher.parameters():\n",
        "        p.requires_grad = False  # Đóng băng các tham số của mô hình giáo viên.\n",
        "\n",
        "    best_prec = 0  # Lưu trữ độ chính xác tốt nhất.\n",
        "    last_epoch = 0  # Lưu trữ epoch cuối cùng đã huấn luyện.\n",
        "\n",
        "    # Resume checkpoint nếu có\n",
        "    if cfg[\"saver\"].get(\"auto_resume\", False):\n",
        "        lastest_model = os.path.join(cfg[\"save_path\"], \"ckpt.pth\")\n",
        "        if not os.path.exists(lastest_model):\n",
        "            \"No checkpoint found in '{}'\".format(lastest_model)\n",
        "        else:\n",
        "            print(f\"Resume model from: '{lastest_model}'\")\n",
        "            best_prec, last_epoch = load_state(\n",
        "                lastest_model, model, optimizer=optimizer, key=\"model_state\"\n",
        "            )\n",
        "            _, _ = load_state(\n",
        "                lastest_model, model_teacher, optimizer=optimizer, key=\"teacher_state\"\n",
        "            )\n",
        "\n",
        "    elif cfg[\"saver\"].get(\"pretrain\", False):  # Nếu có pretrain model:\n",
        "        load_state(cfg[\"saver\"][\"pretrain\"], model, key=\"model_state\")\n",
        "        load_state(cfg[\"saver\"][\"pretrain\"], model_teacher, key=\"teacher_state\")\n",
        "\n",
        "    optimizer_start = get_optimizer(params_list, cfg_optim)  # Khởi tạo optimizer mới.\n",
        "    lr_scheduler = get_scheduler(\n",
        "        cfg_trainer, len(train_loader_sup), optimizer_start, start_epoch=last_epoch\n",
        "    )  # Khởi tạo scheduler cho learning rate.\n",
        "\n",
        "    # Tạo memory bank cho contrastive learning\n",
        "    memobank = []\n",
        "    queue_ptrlis = []\n",
        "    queue_size = []\n",
        "    for i in range(cfg[\"net\"][\"num_classes\"]):  # Tạo memory bank cho từng lớp.\n",
        "        memobank.append([torch.zeros(0, 256)])\n",
        "        queue_size.append(30000)  # Kích thước hàng đợi mặc định.\n",
        "        queue_ptrlis.append(torch.zeros(1, dtype=torch.long))\n",
        "    queue_size[0] = 50000  # Tăng kích thước hàng đợi cho lớp đầu tiên.\n",
        "\n",
        "    # Tạo prototype cho contrastive learning\n",
        "    prototype = torch.zeros(\n",
        "        (\n",
        "            cfg[\"net\"][\"num_classes\"],\n",
        "            cfg[\"trainer\"][\"contrastive\"][\"num_queries\"],\n",
        "            1,\n",
        "            256,\n",
        "        )\n",
        "    ).cuda()\n",
        "\n",
        "    # Bắt đầu huấn luyện\n",
        "    for epoch in range(last_epoch, cfg_trainer[\"epochs\"]):\n",
        "        # Huấn luyện\n",
        "        train(\n",
        "            model,\n",
        "            model_teacher,\n",
        "            optimizer,\n",
        "            lr_scheduler,\n",
        "            sup_loss_fn,\n",
        "            train_loader_sup,\n",
        "            train_loader_unsup,\n",
        "            epoch,\n",
        "            tb_logger,\n",
        "            logger,\n",
        "            memobank,\n",
        "            queue_ptrlis,\n",
        "            queue_size,\n",
        "        )\n",
        "\n",
        "        # Validation\n",
        "        if cfg_trainer[\"eval_on\"]:\n",
        "            if rank == 0:\n",
        "                logger.info(\"start evaluation\")\n",
        "\n",
        "            if epoch < cfg[\"trainer\"].get(\"sup_only_epoch\", 1):  # Nếu đang trong giai đoạn chỉ huấn luyện có nhãn:\n",
        "                prec = validate(model, val_loader, epoch, logger)\n",
        "            else:  # Sau đó sử dụng mô hình giáo viên để đánh giá.\n",
        "                prec = validate(model_teacher, val_loader, epoch, logger)\n",
        "\n",
        "            if rank == 0:\n",
        "                state = {\n",
        "                    \"epoch\": epoch + 1,\n",
        "                    \"model_state\": model.state_dict(),\n",
        "                    \"optimizer_state\": optimizer.state_dict(),\n",
        "                    \"teacher_state\": model_teacher.state_dict(),\n",
        "                    \"best_miou\": best_prec,\n",
        "                }\n",
        "                if prec > best_prec:  # Lưu checkpoint tốt nhất.\n",
        "                    best_prec = prec\n",
        "                    torch.save(\n",
        "                        state, osp.join(cfg[\"saver\"][\"snapshot_dir\"], \"ckpt_best.pth\")\n",
        "                    )\n",
        "\n",
        "                torch.save(state, osp.join(cfg[\"saver\"][\"snapshot_dir\"], \"ckpt.pth\"))\n",
        "\n",
        "                logger.info(\n",
        "                    \"\\033[31m * Currently, the best val result is: {:.2f}\\033[0m\".format(\n",
        "                        best_prec * 100\n",
        "                    )\n",
        "                )\n",
        "                tb_logger.add_scalar(\"mIoU val\", prec, epoch)\n",
        "\n",
        "\n",
        "def train(\n",
        "    model,\n",
        "    model_teacher,\n",
        "    optimizer,\n",
        "    lr_scheduler,\n",
        "    sup_loss_fn,\n",
        "    loader_l,\n",
        "    loader_u,\n",
        "    epoch,\n",
        "    tb_logger,\n",
        "    logger,\n",
        "    memobank,\n",
        "    queue_ptrlis,\n",
        "    queue_size,\n",
        "):\n",
        "    global prototype\n",
        "    ema_decay_origin = cfg[\"net\"][\"ema_decay\"]\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    loader_l.sampler.set_epoch(epoch)\n",
        "    loader_u.sampler.set_epoch(epoch)\n",
        "    loader_l_iter = iter(loader_l)\n",
        "    loader_u_iter = iter(loader_u)\n",
        "    assert len(loader_l) == len(\n",
        "        loader_u\n",
        "    ), f\"labeled data {len(loader_l)} unlabeled data {len(loader_u)}, imbalance!\"\n",
        "\n",
        "    rank, world_size = dist.get_rank(), dist.get_world_size()\n",
        "\n",
        "    sup_losses = AverageMeter(10)\n",
        "    uns_losses = AverageMeter(10)\n",
        "    con_losses = AverageMeter(10)\n",
        "    data_times = AverageMeter(10)\n",
        "    batch_times = AverageMeter(10)\n",
        "    learning_rates = AverageMeter(10)\n",
        "\n",
        "    batch_end = time.time()\n",
        "    for step in range(len(loader_l)):\n",
        "        batch_start = time.time()\n",
        "        data_times.update(batch_start - batch_end)\n",
        "\n",
        "        i_iter = epoch * len(loader_l) + step\n",
        "        lr = lr_scheduler.get_lr()\n",
        "        learning_rates.update(lr[0])\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        image_l, label_l = loader_l_iter.next()\n",
        "        batch_size, h, w = label_l.size()\n",
        "        image_l, label_l = image_l.cuda(), label_l.cuda()\n",
        "\n",
        "        image_u, _ = loader_u_iter.next()\n",
        "        image_u = image_u.cuda()\n",
        "\n",
        "        if epoch < cfg[\"trainer\"].get(\"sup_only_epoch\", 1):\n",
        "            contra_flag = \"none\"\n",
        "            # forward\n",
        "            outs = model(image_l)\n",
        "            pred, rep = outs[\"pred\"], outs[\"rep\"]\n",
        "            pred = F.interpolate(pred, (h, w), mode=\"bilinear\", align_corners=True)\n",
        "\n",
        "            # supervised loss\n",
        "            if \"aux_loss\" in cfg[\"net\"].keys():\n",
        "                aux = outs[\"aux\"]\n",
        "                aux = F.interpolate(aux, (h, w), mode=\"bilinear\", align_corners=True)\n",
        "                sup_loss = sup_loss_fn([pred, aux], label_l)\n",
        "            else:\n",
        "                sup_loss = sup_loss_fn(pred, label_l)\n",
        "\n",
        "            model_teacher.train()\n",
        "            _ = model_teacher(image_l)\n",
        "\n",
        "            unsup_loss = 0 * rep.sum()\n",
        "            contra_loss = 0 * rep.sum()\n",
        "        else:\n",
        "            if epoch == cfg[\"trainer\"].get(\"sup_only_epoch\", 1):\n",
        "                # copy student parameters to teacher\n",
        "                with torch.no_grad():\n",
        "                    for t_params, s_params in zip(\n",
        "                        model_teacher.parameters(), model.parameters()\n",
        "                    ):\n",
        "                        t_params.data = s_params.data\n",
        "\n",
        "            # generate pseudo labels first\n",
        "            model_teacher.eval()\n",
        "            pred_u_teacher = model_teacher(image_u)[\"pred\"]\n",
        "            pred_u_teacher = F.interpolate(\n",
        "                pred_u_teacher, (h, w), mode=\"bilinear\", align_corners=True\n",
        "            )\n",
        "            pred_u_teacher = F.softmax(pred_u_teacher, dim=1)\n",
        "            logits_u_aug, label_u_aug = torch.max(pred_u_teacher, dim=1)\n",
        "\n",
        "            # apply strong data augmentation: cutout, cutmix, or classmix\n",
        "            if np.random.uniform(0, 1) < 0.5 and cfg[\"trainer\"][\"unsupervised\"].get(\n",
        "                \"apply_aug\", False\n",
        "            ):\n",
        "                image_u_aug, label_u_aug, logits_u_aug = generate_unsup_data(\n",
        "                    image_u,\n",
        "                    label_u_aug.clone(),\n",
        "                    logits_u_aug.clone(),\n",
        "                    mode=cfg[\"trainer\"][\"unsupervised\"][\"apply_aug\"],\n",
        "                )\n",
        "            else:\n",
        "                image_u_aug = image_u\n",
        "\n",
        "            # forward\n",
        "            num_labeled = len(image_l)\n",
        "            image_all = torch.cat((image_l, image_u_aug))\n",
        "            outs = model(image_all)\n",
        "            pred_all, rep_all = outs[\"pred\"], outs[\"rep\"]\n",
        "            pred_l, pred_u = pred_all[:num_labeled], pred_all[num_labeled:]\n",
        "            pred_l_large = F.interpolate(\n",
        "                pred_l, size=(h, w), mode=\"bilinear\", align_corners=True\n",
        "            )\n",
        "            pred_u_large = F.interpolate(\n",
        "                pred_u, size=(h, w), mode=\"bilinear\", align_corners=True\n",
        "            )\n",
        "\n",
        "            # supervised loss\n",
        "            if \"aux_loss\" in cfg[\"net\"].keys():\n",
        "                aux = outs[\"aux\"][:num_labeled]\n",
        "                aux = F.interpolate(aux, (h, w), mode=\"bilinear\", align_corners=True)\n",
        "                sup_loss = sup_loss_fn([pred_l_large, aux], label_l.clone())\n",
        "            else:\n",
        "                sup_loss = sup_loss_fn(pred_l_large, label_l.clone())\n",
        "\n",
        "            # teacher forward\n",
        "            model_teacher.train()\n",
        "            with torch.no_grad():\n",
        "                out_t = model_teacher(image_all)\n",
        "                pred_all_teacher, rep_all_teacher = out_t[\"pred\"], out_t[\"rep\"]\n",
        "                prob_all_teacher = F.softmax(pred_all_teacher, dim=1)\n",
        "                prob_l_teacher, prob_u_teacher = (\n",
        "                    prob_all_teacher[:num_labeled],\n",
        "                    prob_all_teacher[num_labeled:],\n",
        "                )\n",
        "\n",
        "                pred_u_teacher = pred_all_teacher[num_labeled:]\n",
        "                pred_u_large_teacher = F.interpolate(\n",
        "                    pred_u_teacher, size=(h, w), mode=\"bilinear\", align_corners=True\n",
        "                )\n",
        "\n",
        "            # unsupervised loss\n",
        "            drop_percent = cfg[\"trainer\"][\"unsupervised\"].get(\"drop_percent\", 100)\n",
        "            percent_unreliable = (100 - drop_percent) * (1 - epoch / cfg[\"trainer\"][\"epochs\"])\n",
        "            drop_percent = 100 - percent_unreliable\n",
        "            unsup_loss = (\n",
        "                    compute_unsupervised_loss(\n",
        "                        pred_u_large,\n",
        "                        label_u_aug.clone(),\n",
        "                        drop_percent,\n",
        "                        pred_u_large_teacher.detach(),\n",
        "                    )\n",
        "                    * cfg[\"trainer\"][\"unsupervised\"].get(\"loss_weight\", 1)\n",
        "            )\n",
        "\n",
        "            # contrastive loss using unreliable pseudo labels\n",
        "            contra_flag = \"none\"\n",
        "            if cfg[\"trainer\"].get(\"contrastive\", False):\n",
        "                cfg_contra = cfg[\"trainer\"][\"contrastive\"]\n",
        "                contra_flag = \"{}:{}\".format(\n",
        "                    cfg_contra[\"low_rank\"], cfg_contra[\"high_rank\"]\n",
        "                )\n",
        "                alpha_t = cfg_contra[\"low_entropy_threshold\"] * (\n",
        "                    1 - epoch / cfg[\"trainer\"][\"epochs\"]\n",
        "                )\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    prob = torch.softmax(pred_u_large_teacher, dim=1)\n",
        "                    entropy = -torch.sum(prob * torch.log(prob + 1e-10), dim=1)\n",
        "\n",
        "                    low_thresh = np.percentile(\n",
        "                        entropy[label_u_aug != 255].cpu().numpy().flatten(), alpha_t\n",
        "                    )\n",
        "                    low_entropy_mask = (\n",
        "                        entropy.le(low_thresh).float() * (label_u_aug != 255).bool()\n",
        "                    )\n",
        "\n",
        "                    high_thresh = np.percentile(\n",
        "                        entropy[label_u_aug != 255].cpu().numpy().flatten(),\n",
        "                        100 - alpha_t,\n",
        "                    )\n",
        "                    high_entropy_mask = (\n",
        "                        entropy.ge(high_thresh).float() * (label_u_aug != 255).bool()\n",
        "                    )\n",
        "\n",
        "                    low_mask_all = torch.cat(\n",
        "                        (\n",
        "                            (label_l.unsqueeze(1) != 255).float(),\n",
        "                            low_entropy_mask.unsqueeze(1),\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                    low_mask_all = F.interpolate(\n",
        "                        low_mask_all, size=pred_all.shape[2:], mode=\"nearest\"\n",
        "                    )\n",
        "                    # down sample\n",
        "\n",
        "                    if cfg_contra.get(\"negative_high_entropy\", True):\n",
        "                        contra_flag += \" high\"\n",
        "                        high_mask_all = torch.cat(\n",
        "                            (\n",
        "                                (label_l.unsqueeze(1) != 255).float(),\n",
        "                                high_entropy_mask.unsqueeze(1),\n",
        "                            )\n",
        "                        )\n",
        "                    else:\n",
        "                        contra_flag += \" low\"\n",
        "                        high_mask_all = torch.cat(\n",
        "                            (\n",
        "                                (label_l.unsqueeze(1) != 255).float(),\n",
        "                                torch.ones(logits_u_aug.shape)\n",
        "                                .float()\n",
        "                                .unsqueeze(1)\n",
        "                                .cuda(),\n",
        "                            ),\n",
        "                        )\n",
        "                    high_mask_all = F.interpolate(\n",
        "                        high_mask_all, size=pred_all.shape[2:], mode=\"nearest\"\n",
        "                    )  # down sample\n",
        "\n",
        "                    # down sample and concat\n",
        "                    label_l_small = F.interpolate(\n",
        "                        label_onehot(label_l, cfg[\"net\"][\"num_classes\"]),\n",
        "                        size=pred_all.shape[2:],\n",
        "                        mode=\"nearest\",\n",
        "                    )\n",
        "                    label_u_small = F.interpolate(\n",
        "                        label_onehot(label_u_aug, cfg[\"net\"][\"num_classes\"]),\n",
        "                        size=pred_all.shape[2:],\n",
        "                        mode=\"nearest\",\n",
        "                    )\n",
        "\n",
        "                if cfg_contra.get(\"binary\", False):\n",
        "                    contra_flag += \" BCE\"\n",
        "                    contra_loss = compute_binary_memobank_loss(\n",
        "                        rep_all,\n",
        "                        torch.cat((label_l_small, label_u_small)).long(),\n",
        "                        low_mask_all,\n",
        "                        high_mask_all,\n",
        "                        prob_all_teacher.detach(),\n",
        "                        cfg_contra,\n",
        "                        memobank,\n",
        "                        queue_ptrlis,\n",
        "                        queue_size,\n",
        "                        rep_all_teacher.detach(),\n",
        "                    )\n",
        "                else:\n",
        "                    if not cfg_contra.get(\"anchor_ema\", False):\n",
        "                        new_keys, contra_loss = compute_contra_memobank_loss(\n",
        "                            rep_all,\n",
        "                            label_l_small.long(),\n",
        "                            label_u_small.long(),\n",
        "                            prob_l_teacher.detach(),\n",
        "                            prob_u_teacher.detach(),\n",
        "                            low_mask_all,\n",
        "                            high_mask_all,\n",
        "                            cfg_contra,\n",
        "                            memobank,\n",
        "                            queue_ptrlis,\n",
        "                            queue_size,\n",
        "                            rep_all_teacher.detach(),\n",
        "                        )\n",
        "                    else:\n",
        "                        prototype, new_keys, contra_loss = compute_contra_memobank_loss(\n",
        "                            rep_all,\n",
        "                            label_l_small.long(),\n",
        "                            label_u_small.long(),\n",
        "                            prob_l_teacher.detach(),\n",
        "                            prob_u_teacher.detach(),\n",
        "                            low_mask_all,\n",
        "                            high_mask_all,\n",
        "                            cfg_contra,\n",
        "                            memobank,\n",
        "                            queue_ptrlis,\n",
        "                            queue_size,\n",
        "                            rep_all_teacher.detach(),\n",
        "                            prototype,\n",
        "                        )\n",
        "\n",
        "                dist.all_reduce(contra_loss)\n",
        "                contra_loss = (\n",
        "                    contra_loss\n",
        "                    / world_size\n",
        "                    * cfg[\"trainer\"][\"contrastive\"].get(\"loss_weight\", 1)\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                contra_loss = 0 * rep_all.sum()\n",
        "\n",
        "        loss = sup_loss + unsup_loss + contra_loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # update teacher model with EMA\n",
        "        if epoch >= cfg[\"trainer\"].get(\"sup_only_epoch\", 1):\n",
        "            with torch.no_grad():\n",
        "                ema_decay = min(\n",
        "                    1\n",
        "                    - 1\n",
        "                    / (\n",
        "                        i_iter\n",
        "                        - len(loader_l) * cfg[\"trainer\"].get(\"sup_only_epoch\", 1)\n",
        "                        + 1\n",
        "                    ),\n",
        "                    ema_decay_origin,\n",
        "                )\n",
        "                for t_params, s_params in zip(\n",
        "                    model_teacher.parameters(), model.parameters()\n",
        "                ):\n",
        "                    t_params.data = (\n",
        "                        ema_decay * t_params.data + (1 - ema_decay) * s_params.data\n",
        "                    )\n",
        "\n",
        "        # gather all loss from different gpus\n",
        "        reduced_sup_loss = sup_loss.clone().detach()\n",
        "        dist.all_reduce(reduced_sup_loss)\n",
        "        sup_losses.update(reduced_sup_loss.item())\n",
        "\n",
        "        reduced_uns_loss = unsup_loss.clone().detach()\n",
        "        dist.all_reduce(reduced_uns_loss)\n",
        "        uns_losses.update(reduced_uns_loss.item())\n",
        "\n",
        "        reduced_con_loss = contra_loss.clone().detach()\n",
        "        dist.all_reduce(reduced_con_loss)\n",
        "        con_losses.update(reduced_con_loss.item())\n",
        "\n",
        "        batch_end = time.time()\n",
        "        batch_times.update(batch_end - batch_start)\n",
        "\n",
        "        if i_iter % 10 == 0 and rank == 0:\n",
        "            logger.info(\n",
        "                \"[{}][{}] \"\n",
        "                \"Iter [{}/{}]\\t\"\n",
        "                \"Data {data_time.val:.2f} ({data_time.avg:.2f})\\t\"\n",
        "                \"Time {batch_time.val:.2f} ({batch_time.avg:.2f})\\t\"\n",
        "                \"Sup {sup_loss.val:.3f} ({sup_loss.avg:.3f})\\t\"\n",
        "                \"Uns {uns_loss.val:.3f} ({uns_loss.avg:.3f})\\t\"\n",
        "                \"Con {con_loss.val:.3f} ({con_loss.avg:.3f})\\t\"\n",
        "                \"LR {lr.val:.5f}\".format(\n",
        "                    cfg[\"dataset\"][\"n_sup\"],\n",
        "                    contra_flag,\n",
        "                    i_iter,\n",
        "                    cfg[\"trainer\"][\"epochs\"] * len(loader_l),\n",
        "                    data_time=data_times,\n",
        "                    batch_time=batch_times,\n",
        "                    sup_loss=sup_losses,\n",
        "                    uns_loss=uns_losses,\n",
        "                    con_loss=con_losses,\n",
        "                    lr=learning_rates,\n",
        "                )\n",
        "            )\n",
        "\n",
        "            tb_logger.add_scalar(\"lr\", learning_rates.val, i_iter)\n",
        "            tb_logger.add_scalar(\"Sup Loss\", sup_losses.val, i_iter)\n",
        "            tb_logger.add_scalar(\"Uns Loss\", uns_losses.val, i_iter)\n",
        "            tb_logger.add_scalar(\"Con Loss\", con_losses.val, i_iter)\n",
        "\n",
        "\n",
        "def validate(\n",
        "    model,\n",
        "    data_loader,\n",
        "    epoch,\n",
        "    logger,\n",
        "):\n",
        "    model.eval()\n",
        "    data_loader.sampler.set_epoch(epoch)\n",
        "\n",
        "    num_classes, ignore_label = (\n",
        "        cfg[\"net\"][\"num_classes\"],\n",
        "        cfg[\"dataset\"][\"ignore_label\"],\n",
        "    )\n",
        "    rank, world_size = dist.get_rank(), dist.get_world_size()\n",
        "\n",
        "    intersection_meter = AverageMeter()\n",
        "    union_meter = AverageMeter()\n",
        "\n",
        "    for step, batch in enumerate(data_loader):\n",
        "        images, labels = batch\n",
        "        images = images.cuda()\n",
        "        labels = labels.long().cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outs = model(images)\n",
        "\n",
        "        # get the output produced by model_teacher\n",
        "        output = outs[\"pred\"]\n",
        "        output = F.interpolate(\n",
        "            output, labels.shape[1:], mode=\"bilinear\", align_corners=True\n",
        "        )\n",
        "        output = output.data.max(1)[1].cpu().numpy()\n",
        "        target_origin = labels.cpu().numpy()\n",
        "\n",
        "        # start to calculate miou\n",
        "        intersection, union, target = intersectionAndUnion(\n",
        "            output, target_origin, num_classes, ignore_label\n",
        "        )\n",
        "\n",
        "        # gather all validation information\n",
        "        reduced_intersection = torch.from_numpy(intersection).cuda()\n",
        "        reduced_union = torch.from_numpy(union).cuda()\n",
        "        reduced_target = torch.from_numpy(target).cuda()\n",
        "\n",
        "        dist.all_reduce(reduced_intersection)\n",
        "        dist.all_reduce(reduced_union)\n",
        "        dist.all_reduce(reduced_target)\n",
        "\n",
        "        intersection_meter.update(reduced_intersection.cpu().numpy())\n",
        "        union_meter.update(reduced_union.cpu().numpy())\n",
        "\n",
        "    iou_class = intersection_meter.sum / (union_meter.sum + 1e-10)\n",
        "    mIoU = np.mean(iou_class)\n",
        "\n",
        "    if rank == 0:\n",
        "        for i, iou in enumerate(iou_class):\n",
        "            logger.info(\" * class [{}] IoU {:.2f}\".format(i, iou * 100))\n",
        "        logger.info(\" * epoch {} mIoU {:.2f}\".format(epoch, mIoU * 100))\n",
        "\n",
        "    return mIoU\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "P0gugdOilFil",
        "outputId": "2c2f6fe0-d2de-4d0e-8fc5-704268ba3b5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorboardX'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-868e072b22f7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboardX\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mu2pl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugmentation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgenerate_unsup_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorboardX'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "execution_count": 1
    }
  ]
}